{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "informational-bangladesh",
   "metadata": {},
   "source": [
    "# Beer prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "operational-slide",
   "metadata": {},
   "source": [
    "!python -c \"import pandas as pd; print(pd.__version__)\"## Set up environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "suspended-patrol",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.2.2\n"
     ]
    }
   ],
   "source": [
    "!python -c \"import pandas as pd; print(pd.__version__)\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "hired-investigation",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import modules\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from joblib import dump\n",
    "from joblib import load\n",
    "from pandas_profiling import ProfileReport\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "together-season",
   "metadata": {},
   "source": [
    "## Get data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "worst-postage",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load magic command for auto-reloading external modules\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "noble-warner",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Turn off table squishing\n",
    "pd.options.display.max_columns = None\n",
    "\n",
    "# Import data\n",
    "df = pd.read_csv(\"../data/raw/beer_reviews.csv\")\n",
    "\n",
    "# Check data\n",
    "#display(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "affected-scroll",
   "metadata": {},
   "source": [
    "## Check data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "internal-manner",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate Data Dictionary\n",
    "# Direct copy & paste from Assignment 2 Brief\n",
    "\n",
    "DataDict = \\\n",
    "    { \"brewery_id\": \"Identifier of brewery\"\n",
    "    , \"brewery_name\": \"Name of brewery\"\n",
    "    , \"review_time\": \"Timestamp of review\"\n",
    "    , \"review_overall\": \"Overall score given by reviewer\"\n",
    "    , \"review_aroma\": \"Score given by reviewer regarding beer aroma\"\n",
    "    , \"review_appearance\": \"Score given by reviewer regarding beer appearance\"\n",
    "    , \"review_profilename\": \"Profile name of reviewer\"\n",
    "    , \"review_palate\": \"Score given by reviewer regarding beer palate\"\n",
    "    , \"review_taste\": \"Score given by reviewer regarding beer taste\"\n",
    "    , \"beer_style (target)\": \"Type of beer\"\n",
    "    , \"beer_name\": \"Name of beer\"\n",
    "    , \"beer_abv\": \"Alcohol by volume measure\"\n",
    "    , \"beer_beerid\": \"Identifier of beer\"\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "negative-korean",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1586614 entries, 0 to 1586613\n",
      "Data columns (total 13 columns):\n",
      " #   Column              Non-Null Count    Dtype  \n",
      "---  ------              --------------    -----  \n",
      " 0   brewery_id          1586614 non-null  int64  \n",
      " 1   brewery_name        1586599 non-null  object \n",
      " 2   review_time         1586614 non-null  int64  \n",
      " 3   review_overall      1586614 non-null  float64\n",
      " 4   review_aroma        1586614 non-null  float64\n",
      " 5   review_appearance   1586614 non-null  float64\n",
      " 6   review_profilename  1586266 non-null  object \n",
      " 7   beer_style          1586614 non-null  object \n",
      " 8   review_palate       1586614 non-null  float64\n",
      " 9   review_taste        1586614 non-null  float64\n",
      " 10  beer_name           1586614 non-null  object \n",
      " 11  beer_abv            1518829 non-null  float64\n",
      " 12  beer_beerid         1586614 non-null  int64  \n",
      "dtypes: float64(6), int64(3), object(4)\n",
      "memory usage: 157.4+ MB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "spanish-rouge",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>brewery_id</th>\n",
       "      <th>review_time</th>\n",
       "      <th>review_overall</th>\n",
       "      <th>review_aroma</th>\n",
       "      <th>review_appearance</th>\n",
       "      <th>review_palate</th>\n",
       "      <th>review_taste</th>\n",
       "      <th>beer_abv</th>\n",
       "      <th>beer_beerid</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>1.586614e+06</td>\n",
       "      <td>1.586614e+06</td>\n",
       "      <td>1.586614e+06</td>\n",
       "      <td>1.586614e+06</td>\n",
       "      <td>1.586614e+06</td>\n",
       "      <td>1.586614e+06</td>\n",
       "      <td>1.586614e+06</td>\n",
       "      <td>1.518829e+06</td>\n",
       "      <td>1.586614e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>3.130099e+03</td>\n",
       "      <td>1.224089e+09</td>\n",
       "      <td>3.815581e+00</td>\n",
       "      <td>3.735636e+00</td>\n",
       "      <td>3.841642e+00</td>\n",
       "      <td>3.743701e+00</td>\n",
       "      <td>3.792860e+00</td>\n",
       "      <td>7.042387e+00</td>\n",
       "      <td>2.171279e+04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>5.578104e+03</td>\n",
       "      <td>7.654427e+07</td>\n",
       "      <td>7.206219e-01</td>\n",
       "      <td>6.976167e-01</td>\n",
       "      <td>6.160928e-01</td>\n",
       "      <td>6.822184e-01</td>\n",
       "      <td>7.319696e-01</td>\n",
       "      <td>2.322526e+00</td>\n",
       "      <td>2.181834e+04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>8.406720e+08</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>1.000000e-02</td>\n",
       "      <td>3.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>1.430000e+02</td>\n",
       "      <td>1.173224e+09</td>\n",
       "      <td>3.500000e+00</td>\n",
       "      <td>3.500000e+00</td>\n",
       "      <td>3.500000e+00</td>\n",
       "      <td>3.500000e+00</td>\n",
       "      <td>3.500000e+00</td>\n",
       "      <td>5.200000e+00</td>\n",
       "      <td>1.717000e+03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>4.290000e+02</td>\n",
       "      <td>1.239203e+09</td>\n",
       "      <td>4.000000e+00</td>\n",
       "      <td>4.000000e+00</td>\n",
       "      <td>4.000000e+00</td>\n",
       "      <td>4.000000e+00</td>\n",
       "      <td>4.000000e+00</td>\n",
       "      <td>6.500000e+00</td>\n",
       "      <td>1.390600e+04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>2.372000e+03</td>\n",
       "      <td>1.288568e+09</td>\n",
       "      <td>4.500000e+00</td>\n",
       "      <td>4.000000e+00</td>\n",
       "      <td>4.000000e+00</td>\n",
       "      <td>4.000000e+00</td>\n",
       "      <td>4.500000e+00</td>\n",
       "      <td>8.500000e+00</td>\n",
       "      <td>3.944100e+04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>2.800300e+04</td>\n",
       "      <td>1.326285e+09</td>\n",
       "      <td>5.000000e+00</td>\n",
       "      <td>5.000000e+00</td>\n",
       "      <td>5.000000e+00</td>\n",
       "      <td>5.000000e+00</td>\n",
       "      <td>5.000000e+00</td>\n",
       "      <td>5.770000e+01</td>\n",
       "      <td>7.731700e+04</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         brewery_id   review_time  review_overall  review_aroma  \\\n",
       "count  1.586614e+06  1.586614e+06    1.586614e+06  1.586614e+06   \n",
       "mean   3.130099e+03  1.224089e+09    3.815581e+00  3.735636e+00   \n",
       "std    5.578104e+03  7.654427e+07    7.206219e-01  6.976167e-01   \n",
       "min    1.000000e+00  8.406720e+08    0.000000e+00  1.000000e+00   \n",
       "25%    1.430000e+02  1.173224e+09    3.500000e+00  3.500000e+00   \n",
       "50%    4.290000e+02  1.239203e+09    4.000000e+00  4.000000e+00   \n",
       "75%    2.372000e+03  1.288568e+09    4.500000e+00  4.000000e+00   \n",
       "max    2.800300e+04  1.326285e+09    5.000000e+00  5.000000e+00   \n",
       "\n",
       "       review_appearance  review_palate  review_taste      beer_abv  \\\n",
       "count       1.586614e+06   1.586614e+06  1.586614e+06  1.518829e+06   \n",
       "mean        3.841642e+00   3.743701e+00  3.792860e+00  7.042387e+00   \n",
       "std         6.160928e-01   6.822184e-01  7.319696e-01  2.322526e+00   \n",
       "min         0.000000e+00   1.000000e+00  1.000000e+00  1.000000e-02   \n",
       "25%         3.500000e+00   3.500000e+00  3.500000e+00  5.200000e+00   \n",
       "50%         4.000000e+00   4.000000e+00  4.000000e+00  6.500000e+00   \n",
       "75%         4.000000e+00   4.000000e+00  4.500000e+00  8.500000e+00   \n",
       "max         5.000000e+00   5.000000e+00  5.000000e+00  5.770000e+01   \n",
       "\n",
       "        beer_beerid  \n",
       "count  1.586614e+06  \n",
       "mean   2.171279e+04  \n",
       "std    2.181834e+04  \n",
       "min    3.000000e+00  \n",
       "25%    1.717000e+03  \n",
       "50%    1.390600e+04  \n",
       "75%    3.944100e+04  \n",
       "max    7.731700e+04  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "gross-decimal",
   "metadata": {},
   "outputs": [],
   "source": [
    "# View data profile\n",
    "\n",
    "# Create profile report\n",
    "profile = ProfileReport(df, title=\"Profile Report\", minimal=True)\n",
    "# Export\n",
    "profile.to_file('../reports/InitialReport.html')\n",
    "# View\n",
    "#profile"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "light-object",
   "metadata": {},
   "source": [
    "## Prepare data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "general-dining",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a copy of df and save it into a variable called df_cleaned\n",
    "df_cleaned = df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "guilty-admission",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dictionary called cats_dict that contains the categorical variables as keys and their respective values sorted in ascending order\n",
    "cats_dict = {\n",
    "    'brewery_name': [[]],\n",
    "    'review_profilename': [[]],\n",
    "    'beer_style': [[]],\n",
    "    'beer_name': [[]],\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "strong-closing",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_col = 'beer_style'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "falling-swiss",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import StandardScaler and OrdinalEncoder from sklearn.preprocessing\n",
    "from sklearn.preprocessing import StandardScaler, OrdinalEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "central-messaging",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iterate through the elements of cast_dict, instantiate an OrdinalEncoder() and transform the values of each column with this encoder\n",
    "for col, cats in cats_dict.items():\n",
    "    col_encoder = OrdinalEncoder(categories=cats)\n",
    "    df_cleaned[col] = col_encoder.fit_transform(df_cleaned[[col]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "hundred-morgan",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a list called num_cols that contains all numeric columns\n",
    "num_cols = ['brewery_id', 'review_time', 'review_overall', 'review_aroma', 'review_appearance', 'review_palate', 'review_taste', 'beer_abv', 'beer_beerid']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cordless-torture",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate a StandardScaler and called it sc\n",
    "sc = StandardScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "loaded-scope",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit and transform the numeric feature of X_train_cleaned and replace the data into it\n",
    "df_cleaned[num_cols] = sc.fit_transform(df_cleaned[num_cols])"
   ]
  },
  {
   "cell_type": "raw",
   "id": "configured-fundamental",
   "metadata": {},
   "source": [
    "#Convert the column evaluation as integer\n",
    "df_cleaned['evaluation'] = df_cleaned['evaluation'].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "invalid-cable",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'split_sets_by_time'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-13-543edc067fb9>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0msplit_sets_by_time\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msave_sets\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'split_sets_by_time'"
     ]
    }
   ],
   "source": [
    "from src.data.sets import split_sets_by_time, save_sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "destroyed-century",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.data.sets import split_sets_by_time, save_sets\n",
    "\n",
    "# Split data into training and testing with 80-20 ratio\n",
    "X_train, y_train, X_val, y_val, X_test, y_test = split_sets_random(X, target_col=target_col, test_ratio=0.2, to_numpy=True)\n",
    "\n",
    "# Save sets to folder\n",
    "save_sets(X_train=X_train, y_train=y_train, X_val=X_val, y_val=y_val, X_test=X_test, y_test=y_test, path='../data/processed/credit_card_default/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "egyptian-crowd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import this class from src/models/pytorch and convert all sets to PytorchDataset\n",
    "from src.models.pytorch import PytorchDataset\n",
    "\n",
    "train_dataset = PytorchDataset(X=X_train, y=y_train)\n",
    "val_dataset = PytorchDataset(X=X_val, y=y_val)\n",
    "test_dataset = PytorchDataset(X=X_test, y=y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "authorized-nebraska",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import NullModel from src.models.null\n",
    "from src.models.null import NullModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "british-continuity",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate a NullModel and call .fit_predict() on the training target to extract your predictions into a variable called y_base\n",
    "baseline_model = NullModel(target_type='classification')\n",
    "y_base = baseline_model.fit_predict(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "asian-airport",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import print_class_perf from src.models.performance\n",
    "from src.models.performance import print_class_perf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "optimum-franchise",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Task: Print the classification metrics for this baseline model\n",
    "print_class_perf(y_base, y_train, set_name='Training', average='weighted')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "heard-freeware",
   "metadata": {},
   "source": [
    "## Define architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "coordinate-minneapolis",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import torch and torch.nn as nn\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "naked-collar",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a class called PytorchMultiClass that inherits from nn.Module\n",
    "class PytorchMultiClass(nn.Module):\n",
    "    def __init__(self, num_features):\n",
    "        super(PytorchMultiClass, self).__init__()\n",
    "        \n",
    "        self.layer_1 = nn.Linear(num_features, 32)\n",
    "        self.layer_out = nn.Linear(32, 4)\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.dropout(F.relu(self.layer_1(x)), training=self.training)\n",
    "        x = self.layer_out(x)\n",
    "        return self.softmax(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "convenient-magnet",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate PytorchMultiClass with the correct number of input feature and save it into a variable called model\n",
    "from src.models.pytorch import PytorchMultiClass\n",
    "model = PytorchMultiClass(X_train.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dramatic-technology",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set model to use the device available\n",
    "from src.models.pytorch import get_device\n",
    "device = get_device()\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "behind-episode",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the architecture of model\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "female-invite",
   "metadata": {},
   "source": [
    "## Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "educational-favor",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate a nn.CrossEntropyLoss() and save it into a variable called criterion\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "human-characteristic",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate a torch.optim.Adam() optimizer with the model's parameters and 0.1 as learning rate and save it into a variable called optimizer\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "modular-scholarship",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a function called train_classification() that will perform forward and back propagation and calculate loss and Accuracy scores\n",
    "def train_classification(train_data, model, criterion, optimizer, batch_size, device, scheduler=None, generate_batch=None):\n",
    "    \"\"\"Train a Pytorch multi-class classification model\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    train_data : torch.utils.data.Dataset\n",
    "        Pytorch dataset\n",
    "    model: torch.nn.Module\n",
    "        Pytorch Model\n",
    "    criterion: function\n",
    "        Loss function\n",
    "    optimizer: torch.optim\n",
    "        Optimizer\n",
    "    bacth_size : int\n",
    "        Number of observations per batch\n",
    "    device : str\n",
    "        Name of the device used for the model\n",
    "    scheduler : torch.optim.lr_scheduler\n",
    "        Pytorch Scheduler used for updating learning rate\n",
    "    collate_fn : function\n",
    "        Function defining required pre-processing steps\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    Float\n",
    "        Loss score\n",
    "    Float:\n",
    "        Accuracy Score\n",
    "    \"\"\"\n",
    "    \n",
    "    # Set model to training mode\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    train_acc = 0\n",
    "    \n",
    "    # Create data loader\n",
    "    data = DataLoader(train_data, batch_size=batch_size, shuffle=True, collate_fn=generate_batch)\n",
    "    \n",
    "    # Iterate through data by batch of observations\n",
    "    for feature, target_class in data:\n",
    "\n",
    "        # Reset gradients\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Load data to specified device\n",
    "        feature, target_class = feature.to(device), target_class.to(device)\n",
    "        \n",
    "        # Make predictions\n",
    "        output = model(feature)\n",
    "        \n",
    "        # Calculate loss for given batch\n",
    "        loss = criterion(output, target_class.long())\n",
    "\n",
    "        # Calculate global loss\n",
    "        train_loss += loss.item()\n",
    "        \n",
    "        # Calculate gradients\n",
    "        loss.backward()\n",
    "\n",
    "        # Update Weights\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Calculate global accuracy\n",
    "        train_acc += (output.argmax(1) == target_class).sum().item()\n",
    "\n",
    "    # Adjust the learning rate\n",
    "    if scheduler:\n",
    "        scheduler.step()\n",
    "\n",
    "    return train_loss / len(train_data), train_acc / len(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "stock-defendant",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a function called test_classification() that will perform forward and calculate loss and accuracy scores\n",
    "def test_classification(test_data, model, criterion, batch_size, device, generate_batch=None):\n",
    "    \"\"\"Calculate performance of a Pytorch multi-class classification model\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    test_data : torch.utils.data.Dataset\n",
    "        Pytorch dataset\n",
    "    model: torch.nn.Module\n",
    "        Pytorch Model\n",
    "    criterion: function\n",
    "        Loss function\n",
    "    bacth_size : int\n",
    "        Number of observations per batch\n",
    "    device : str\n",
    "        Name of the device used for the model\n",
    "    collate_fn : function\n",
    "        Function defining required pre-processing steps\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    Float\n",
    "        Loss score\n",
    "    Float:\n",
    "        Accuracy Score\n",
    "    \"\"\"    \n",
    "    \n",
    "    # Set model to evaluation mode\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    test_acc = 0\n",
    "    \n",
    "    # Create data loader\n",
    "    data = DataLoader(test_data, batch_size=batch_size, collate_fn=generate_batch)\n",
    "    \n",
    "    # Iterate through data by batch of observations\n",
    "    for feature, target_class in data:\n",
    "        \n",
    "        # Load data to specified device\n",
    "        feature, target_class = feature.to(device), target_class.to(device)\n",
    "        \n",
    "        # Set no update to gradients\n",
    "        with torch.no_grad():\n",
    "            \n",
    "            # Make predictions\n",
    "            output = model(feature)\n",
    "            \n",
    "            # Calculate loss for given batch\n",
    "            loss = criterion(output, target_class.long())\n",
    "\n",
    "            # Calculate global loss\n",
    "            test_loss += loss.item()\n",
    "            \n",
    "            # Calculate global accuracy\n",
    "            test_acc += (output.argmax(1) == target_class).sum().item()\n",
    "\n",
    "    return test_loss / len(test_data), test_acc / len(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "backed-commander",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create 2 variables called N_EPOCHS and BATCH_SIZE that will take respectively 50 and 32 as values\n",
    "N_EPOCHS = 50\n",
    "BATCH_SIZE = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "illegal-banner",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a for loop that will iterate through the specified number of epochs and will train the model with the training set and assess the performance on the validation set and print their scores\n",
    "for epoch in range(N_EPOCHS):\n",
    "    train_loss, train_acc = train_classification(train_dataset, model=model, criterion=criterion, optimizer=optimizer, batch_size=BATCH_SIZE, device=device)\n",
    "    valid_loss, valid_acc = test_classification(val_dataset, model=model, criterion=criterion, batch_size=BATCH_SIZE, device=device)\n",
    "\n",
    "    print(f'Epoch: {epoch}')\n",
    "    print(f'\\t(train)\\t|\\tLoss: {train_loss:.4f}\\t|\\tAcc: {train_acc * 100:.1f}%')\n",
    "    print(f'\\t(valid)\\t|\\tLoss: {valid_loss:.4f}\\t|\\tAcc: {valid_acc * 100:.1f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecological-venture",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model into the models folder\n",
    "torch.save(model, \"../models/pytorch_multi_car_evaluation.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "potential-timeline",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assess the model performance on the testing set and print its scores\n",
    "test_loss, test_acc = test_classification(test_dataset, model=model, criterion=criterion, batch_size=BATCH_SIZE, device=device)\n",
    "print(f'\\tLoss: {test_loss:.4f}\\t|\\tAccuracy: {test_acc:.1f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "nearby-japan",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ethical-desire",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "direct-slave",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "provincial-humidity",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "promotional-boring",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "romantic-investor",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "finite-campus",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "senior-divide",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop ID column - training and test data\n",
    "df_train = df_train.drop(['Id_old','Id'], axis=1)\n",
    "df_test = df_test.drop(['Id_old','Id'], axis=1)\n",
    "\n",
    "# Identify target\n",
    "features = df_train.iloc[:,:-1].to_numpy()\n",
    "target = df_train.iloc[:,-1].to_numpy()\n",
    "\n",
    "# Standardize features - training data\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "features = scaler.fit_transform(features)\n",
    "\n",
    "# Standardize features - test data\n",
    "X_test = scaler.fit_transform(df_test)\n",
    "\n",
    "# Split into training and validation sets\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_val, y_train, y_val = train_test_split(features, target, test_size=0.1, random_state=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "attached-privilege",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save data objects into folder\n",
    "dump(scaler, '../models/David/scaler.joblib')\n",
    "dump(X_train, '../data/processed/David/X_train.joblib')\n",
    "dump(X_val, '../data/processed/David/X_val.joblib')\n",
    "dump(y_train, '../data/processed/David/y_train.joblib')\n",
    "dump(y_val, '../data/processed/David/y_val.joblib')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "endangered-agenda",
   "metadata": {},
   "source": [
    "## Experimentation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "internal-electric",
   "metadata": {},
   "source": [
    "Overview\n",
    "\n",
    "We will need to do the following steps:\n",
    "\n",
    "Define custom class for handling Numpy data in to PyTorch Dataset type: BasketballDataset().\n",
    "Define a custom class, inhereted from the nn.Module class, that can allow us to manually create the layers & functions we want: Net().\n",
    "Define custom functions for:\n",
    "Plotting the performance of the Model: plot_network_training().\n",
    "Training the model: model_train().\n",
    "Validating the model: model_validate().\n",
    "Running the overall epochs: train_overall_network().\n",
    "Run the training and review the learning plots.\n",
    "Run the validation data through the network.\n",
    "Validate the model performance.\n",
    "The layer transformations should be:\n",
    "\n",
    "19→20→30→40→20→10→5→1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "small-robert",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "from torch import optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "# Reproducability\n",
    "torch.manual_seed(123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "proved-indicator",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We need to define a custom Dataset class, so PyTorch can recognise our data... coz it likes to be picky\n",
    "# Inspired by:\n",
    "#   https://towardsdatascience.com/deep-learning-for-tabular-data-using-pytorch-1807f2858320\n",
    "#   https://towardsdatascience.com/better-data-loading-20x-pytorch-speed-up-for-tabular-data-e264b9e34352\n",
    "\n",
    "# Define\n",
    "class BasketballDataset(Dataset):\n",
    "    def __init__(self, feat, targ):\n",
    "        self.feat = feat.copy().astype(np.float32)\n",
    "        self.targ = targ.copy().astype(np.float32)\n",
    "    def __len__(self):\n",
    "        return len(self.targ)\n",
    "    def __getitem__(self, index):\n",
    "        return self.feat[index], self.targ[index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "impaired-somewhere",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspiration from:\n",
    "# https://pytorch.org/tutorials/recipes/recipes/defining_a_neural_network.html\n",
    "\n",
    "# Define net\n",
    "class Net(nn.Module):\n",
    "    \"\"\"Redefine class from torch\"\"\"\n",
    "\n",
    "    # Initalise\n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        Initialise with all precidences.\n",
    "        Then manually define layers to be used later\n",
    "        \"\"\"\n",
    "        \n",
    "        # Imports\n",
    "        import numpy as np\n",
    "\n",
    "        # Super initialisation\n",
    "        super().__init__()\n",
    "\n",
    "        # Fully connected layers\n",
    "        # 19→20→30→40→20→10→5→1\n",
    "        self.fc1 = nn.Linear(19,20)\n",
    "        self.fc2 = nn.Linear(20,30)\n",
    "        self.fc3 = nn.Linear(30,40)\n",
    "        self.fc4 = nn.Linear(40,20)\n",
    "        self.fc5 = nn.Linear(20,10)\n",
    "        self.fc6 = nn.Linear(10,5)\n",
    "        self.fc7 = nn.Linear(5,1)\n",
    "\n",
    "    # Static propagation 😒\n",
    "    def old(self, x):\n",
    "        \"\"\"Run forward prop\"\"\"\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = F.relu(self.fc3(x))\n",
    "        x = F.relu(self.fc4(x))\n",
    "        x = F.relu(self.fc5(x))\n",
    "        x = F.relu(self.fc6(x))\n",
    "        x = F.sigmoid(self.fc7(x))\n",
    "        x = x[:,0]\n",
    "        return x\n",
    "\n",
    "    # Dynamic propagation\n",
    "    def forward \\\n",
    "        ( self\n",
    "        , feat\n",
    "        , hidden_shapes:list=[20,20,20]\n",
    "        , hidden_acti:str=\"relu\"\n",
    "        , final_shape:int=1\n",
    "        , final_acti:str=\"sigmoid\"\n",
    "        ):\n",
    "\n",
    "        # Check\n",
    "        if type(feat)==np.ndarray:\n",
    "            feat = torch.from_numpy(feat.astype(np.float32))\n",
    "        \n",
    "        # Assertions\n",
    "        assert np.all([np.isscalar(param) for param in [hidden_acti, final_shape, final_acti]])\n",
    "        assert isinstance(hidden_shapes, list)\n",
    "        assert len(hidden_shapes)>0, \"Must have at least 1 hidden layer\"\n",
    "        assert np.all([isinstance(elem, int) for elem in hidden_shapes])\n",
    "        assert isinstance(final_shape, int)\n",
    "        assert np.all([isinstance(param, str) for param in [hidden_acti, final_acti]])\n",
    "\n",
    "        # Define number of nodes in input\n",
    "        input_shape=feat.shape[-1]\n",
    "\n",
    "        # Work on first hidden layer\n",
    "        shape = nn.Linear(input_shape, hidden_shapes[0])\n",
    "        x = shape(feat)\n",
    "        x = eval(\"F.{}\".format(hidden_acti))(x)\n",
    "\n",
    "        # Loop other layers\n",
    "        for layer in range(len(hidden_shapes)-1): #<-- `-1` because skip last layer\n",
    "            \n",
    "            # Get shapes\n",
    "            curr_layer_shape = hidden_shapes[layer]\n",
    "            next_layer_shape = hidden_shapes[layer+1]\n",
    "            \n",
    "            # Work on other hidden layers\n",
    "            shape = nn.Linear(curr_layer_shape, next_layer_shape)\n",
    "            x = shape(x)\n",
    "            x = eval(\"F.{}\".format(hidden_acti))(x)\n",
    "\n",
    "        # Work on last hidden layer\n",
    "        shape = nn.Linear(hidden_shapes[-1], final_shape)\n",
    "        x = shape(x)\n",
    "        x = eval(\"F.{}\".format(final_acti))(x)\n",
    "\n",
    "        # Return\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "laughing-script",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_network_training(metrics:dict):\n",
    "    \n",
    "    # Imports\n",
    "    from IPython.display import clear_output\n",
    "    import numpy as np\n",
    "    import matplotlib.pyplot as plt\n",
    "    from src.utils.misc import all_in\n",
    "\n",
    "    # Assertions\n",
    "    assert isinstance(metrics, dict)\n",
    "    assert all_in([\"accu_trn\", \"loss_trn\", \"accu_val\", \"loss_val\"], list(metrics.keys()))\n",
    "\n",
    "    # If only 1 score, then end\n",
    "    epoch = len(next(iter(metrics.values())))\n",
    "    if epoch < 2:\n",
    "        return None\n",
    "\n",
    "    # Clearn previous output\n",
    "    clear_output(wait=True)\n",
    "\n",
    "    # Define space\n",
    "    N = np.arange(1, len(next(iter(metrics.values())))+1)\n",
    "\n",
    "    # You can chose the style of your preference\n",
    "    # print(plt.style.available) to see the available options\n",
    "    #plt.style.use(\"seaborn\")\n",
    "\n",
    "    # Plot train loss, train acc, val loss and val acc against epochs passed\n",
    "    plt.figure(figsize=(8,8))\n",
    "    \n",
    "    # Accuracy\n",
    "    plt.subplot(2,1,1)\n",
    "    plt.plot(N, metrics.get(\"accu_trn\"), label = \"Training Accuracy\")\n",
    "    plt.plot(N, metrics.get(\"accu_val\"), label = \"Validation Accuracy\")\n",
    "    plt.legend(loc=\"best\")\n",
    "    plt.title(\"Accuracy [Epoch {}]\".format(epoch))\n",
    "    plt.ylim([0,1.1])\n",
    "    plt.ylabel(\"Accuracy\")\n",
    "    \n",
    "    # Loss\n",
    "    plt.subplot(2,1,2)\n",
    "    plt.plot(N, metrics.get(\"loss_trn\"), label = \"Training Loss\")\n",
    "    plt.plot(N, metrics.get(\"loss_val\"), label = \"Validation Loss\")\n",
    "    plt.legend(loc=\"best\")\n",
    "    plt.title(\"Loss [Epoch {}]\".format(epoch))\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.xlabel(\"Epoch #\")\n",
    "    \n",
    "    # Show\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baking-lucas",
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_train \\\n",
    "    ( data_trn:torch.utils.data.Dataset\n",
    "    , modl:torch.nn.Module\n",
    "    , crit:torch.nn\n",
    "    , optm:torch.optim\n",
    "    , batch_size:int=100\n",
    "    , hidden_shapes:list=[20,30,40]\n",
    "    , hidden_acti:str=\"relu\"\n",
    "    , final_shape:int=1\n",
    "    , final_acti:str=\"sigmoid\"\n",
    "    , device:torch.device=torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "    , scheduler:bool=True\n",
    "    ):\n",
    "\n",
    "    # Set to train\n",
    "    model.train()\n",
    "    loss_trn = 0.0\n",
    "    accu_trn = 0.0\n",
    "\n",
    "    # Set data generator\n",
    "    load_trn = DataLoader(data_trn, batch_size=batch_size, shuffle=True, num_workers=0)\n",
    "\n",
    "    # Loop over each batch\n",
    "    for batch, data in enumerate(load_trn):\n",
    "        \n",
    "        # Extract data\n",
    "        inputs, labels = data\n",
    "\n",
    "        # Push data to device\n",
    "        # inputs, labels = inputs.to(device), labels.to(device)\n",
    "        inputs.to(device)\n",
    "        labels.to(device)\n",
    "\n",
    "        # Zero out the parameter gradients\n",
    "        optm.zero_grad()\n",
    "\n",
    "        # Feed forward\n",
    "        output = modl.forward \\\n",
    "            ( feat=inputs\n",
    "            , hidden_shapes=hidden_shapes\n",
    "            , hidden_acti=hidden_acti\n",
    "            , final_shape=final_shape\n",
    "            , final_acti=final_acti\n",
    "            )\n",
    "\n",
    "        # Calc loss\n",
    "        loss = crit(output, labels.unsqueeze(1))\n",
    "\n",
    "        # Global metrics\n",
    "        loss_trn += loss.item()\n",
    "        accu_trn += (output.argmax(1) == labels).sum().item()\n",
    "\n",
    "        # Feed backward\n",
    "        loss.backward()\n",
    "\n",
    "        # Optimise\n",
    "        optm.step()\n",
    "\n",
    "    # Adjust scheduler\n",
    "    if scheduler:\n",
    "        scheduler.step()\n",
    "    \n",
    "    return loss_trn/len(data_trn), accu_trn/len(data_trn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "explicit-ordering",
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_validate \\\n",
    "    ( data_val:torch.utils.data.Dataset\n",
    "    , modl:torch.nn.Module\n",
    "    , crit:torch.nn\n",
    "    , batch_size:int=100\n",
    "    , hidden_shapes:list=[20,30,40]\n",
    "    , hidden_acti:str=\"relu\"\n",
    "    , final_shape:int=1\n",
    "    , final_acti:str=\"sigmoid\"\n",
    "    , device:torch.device=torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "    ):\n",
    "\n",
    "    # Set to validation\n",
    "    model.eval()\n",
    "    accu_val = 0\n",
    "    loss_val = 0\n",
    "\n",
    "    # Set generator\n",
    "    load_val = DataLoader(data_val, batch_size=batch_size, shuffle=True, num_workers=0)\n",
    "\n",
    "    # Loop over each batch\n",
    "    for batch, data in enumerate(load_val):\n",
    "        \n",
    "        # Extract data\n",
    "        inputs, labels = data\n",
    "\n",
    "        # Push data to device\n",
    "        # inputs, labels = inputs.to(device), labels.to(device)\n",
    "        inputs.to(device)\n",
    "        labels.to(device)\n",
    "\n",
    "        # Don't update gradients\n",
    "        with torch.no_grad():\n",
    "            \n",
    "            # Make predictions\n",
    "            output = modl \\\n",
    "                ( feat=inputs\n",
    "                , hidden_shapes=hidden_shapes\n",
    "                , hidden_acti=hidden_acti\n",
    "                , final_shape=final_shape\n",
    "                , final_acti=final_acti\n",
    "                )\n",
    "\n",
    "            # Calculate loss\n",
    "            loss = crit(output, labels.unsqueeze(1))\n",
    "\n",
    "            # Global metrics\n",
    "            loss_val += loss.item()\n",
    "            accu_val += (output.argmax(1) == labels).sum().item()\n",
    "    \n",
    "    return loss_val/len(data_val), accu_val/len(data_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "mobile-heart",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_overall_network \\\n",
    "    ( feat:np.real\n",
    "    , targ:np.real\n",
    "    , hidden_shapes:list=[20,20,20]\n",
    "    , hidden_acti:str=\"relu\"\n",
    "    , final_shape:int=1\n",
    "    , final_acti:str=\"sigmoid\"\n",
    "    , batch_size:int=100\n",
    "    , epochs:int=500\n",
    "    , learning_rate:float=0.001\n",
    "    , device:torch.device=torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "    , scheduler:bool=True\n",
    "    , verbosity:int=10\n",
    "    , plot_learning:bool=True\n",
    "    ):\n",
    "\n",
    "    # Imports\n",
    "    import numpy as np\n",
    "    \n",
    "    # Assertions\n",
    "    assert np.all([np.all(np.isreal(param)) for param in [feat, targ]])\n",
    "    assert isinstance(hidden_shapes, list)\n",
    "    assert len(hidden_shapes)>0, \"Must have at least 1 hidden layer\"\n",
    "    assert np.all([isinstance(elem, int) for elem in hidden_shapes])\n",
    "    assert np.all([np.isscalar(param) for param in [hidden_acti, final_shape, final_acti, batch_size, epochs, learning_rate]])\n",
    "    assert isinstance(verbosity, (int, type(None)))\n",
    "    assert np.all([isinstance(param, int) for param in [batch_size, epochs, verbosity]])\n",
    "    assert np.all([isinstance(param, str) for param in [hidden_acti, final_acti]])\n",
    "    assert isinstance(learning_rate, float)\n",
    "\n",
    "    # Initialise data generators\n",
    "    data_trn = BasketballDataset(feat_trn, targ_trn)\n",
    "    data_val = BasketballDataset(feat_val, targ_val)\n",
    "    # load_trn = DataLoader(data_trn, batch_size=batch_size, shuffle=True, num_workers=0)\n",
    "    # load_val = DataLoader(data_val, batch_size=batch_size, shuffle=True, num_workers=0)\n",
    "\n",
    "    # Initialise classes\n",
    "    modl = Net()\n",
    "    crit = nn.BCELoss()\n",
    "    optm = optim.Adam(net.parameters(), lr=learning_rate)\n",
    "    if scheduler:\n",
    "        scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optm)\n",
    "\n",
    "    # Push network to device\n",
    "    net.to(device)\n",
    "    \n",
    "    # Set dumping ground\n",
    "    costs = {\"epoch\": [], \"loss_trn\": [], \"accu_trn\": [], \"loss_val\": [], \"accu_val\": []}\n",
    "    loss_trn = 0.0\n",
    "    accu_trn = 0.0\n",
    "\n",
    "    # Loop over epochs\n",
    "    for epoch in range(epochs):\n",
    "\n",
    "        loss_trn, accu_trn = model_train \\\n",
    "            ( data_trn=data_trn\n",
    "            , modl=modl\n",
    "            , crit=crit\n",
    "            , optm=optm\n",
    "            , batch_size=batch_size\n",
    "            , hidden_shapes=hidden_shapes\n",
    "            , hidden_acti=hidden_acti\n",
    "            , final_shape=final_shape\n",
    "            , final_acti=final_acti\n",
    "            , device=device\n",
    "            , scheduler=scheduler\n",
    "            )\n",
    "        \n",
    "        loss_val, accu_val = model_validate \\\n",
    "            ( data_val=data_val\n",
    "            , modl=modl\n",
    "            , crit=crit\n",
    "            , batch_size=batch_size\n",
    "            , hidden_shapes=hidden_shapes\n",
    "            , hidden_acti=hidden_acti\n",
    "            , final_shape=final_shape\n",
    "            , final_acti=final_acti\n",
    "            , device=device\n",
    "            )\n",
    "\n",
    "        # Record progress\n",
    "        costs[\"epoch\"].append(epoch+1)\n",
    "        costs[\"loss_trn\"].append(loss_trn)\n",
    "        costs[\"accu_trn\"].append(accu_trn)\n",
    "        costs[\"loss_val\"].append(loss_val)\n",
    "        costs[\"accu_val\"].append(accu_val)\n",
    "\n",
    "        # Adjust scheduler\n",
    "        if scheduler:\n",
    "            scheduler.step()\n",
    "\n",
    "        # Print stats\n",
    "        if verbosity:\n",
    "            if epoch % verbosity == 0 or epoch+1==epochs:\n",
    "                # Plot learning\n",
    "                if plot_learning:\n",
    "                    plot_network_training(costs)\n",
    "                # Print metrics\n",
    "                # print(\"Epoch: {}/{}\\tLoss: {:.5f}\".format(costs[\"epoch\"][-1], epochs, costs[\"trn_los\"][-1]))\n",
    "\n",
    "    # Return\n",
    "    return modl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "revised-seven",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = train_overall_network \\\n",
    "    ( feat=feat_trn\n",
    "    , targ=targ_trn\n",
    "    , hidden_shapes=[20,30,40,20,10,5]\n",
    "    , hidden_acti=\"relu\"\n",
    "    , final_shape=1\n",
    "    , final_acti=\"sigmoid\"\n",
    "    , batch_size=100\n",
    "    , epochs=100\n",
    "    , learning_rate=0.00001\n",
    "    , verbosity=10\n",
    "    , scheduler=False\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ceramic-authentication",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "torch.manual_seed(499607495906900)\n",
    "pred_prob = model(feat_val).squeeze(1).detach().numpy()\n",
    "pred = pred_prob.round()\n",
    "pred_scor = save_reg_perf \\\n",
    "    ( targ=targ_val\n",
    "    , pred=pred\n",
    "    , pred_prob=pred_prob\n",
    "    , df_metrics=pred_scor\n",
    "    , name=\"03 - PyTorch - SixLayerModule\"\n",
    "    )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
