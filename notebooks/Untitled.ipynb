{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "brazilian-updating",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "from torchsummary import summary\n",
    "#mport torchvision\n",
    "#rom torchvision import models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "creative-exhibition",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PytorchMultiClass(nn.Module):\n",
    "    def __init__(self, num_features):\n",
    "        super(PytorchMultiClass, self).__init__()\n",
    "        \n",
    "        self.layer_1 = nn.Linear(num_features, 32)\n",
    "        self.layer_2 = nn.Linear(32,16)\n",
    "        self.layer_3 = nn.Linear(16,8)\n",
    "        self.layer_out = nn.Linear(8, 104) #Number of classes in target variable\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.dropout(F.relu(self.layer_1(x)), training=self.training)\n",
    "        x = F.dropout(F.relu(self.layer_2(x)), training=self.training)\n",
    "        x = F.dropout(F.relu(self.layer_3(x)), training=self.training)\n",
    "        x = self.layer_out(x)\n",
    "        return self.softmax(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "characteristic-alfred",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import Tensor\n",
    "from numpy import argmax\n",
    "\n",
    "def predict(brewery_name: int, review_aroma: int, review_appearance:int, review_palate: int, review_taste: int):\n",
    "    features = [brewery_name, review_aroma, review_appearance, review_palate, review_taste]\n",
    "    # convert row to data\n",
    "    features = Tensor([features])\n",
    "    # make prediction\n",
    "    yhat = model(features)\n",
    "    # retrieve numpy array\n",
    "    yhat = yhat.detach().numpy()\n",
    "    return argmax(yhat)\n",
    "    #return JSONResponse(pred.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "biological-attraction",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = torch.load(\"../models/beer_pred.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "satisfied-wages",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Linear-1             [-1, 1000, 32]             192\n",
      "            Linear-2             [-1, 1000, 16]             528\n",
      "            Linear-3              [-1, 1000, 8]             136\n",
      "            Linear-4            [-1, 1000, 104]             936\n",
      "           Softmax-5            [-1, 1000, 104]               0\n",
      "================================================================\n",
      "Total params: 1,792\n",
      "Trainable params: 1,792\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.02\n",
      "Forward/backward pass size (MB): 2.01\n",
      "Params size (MB): 0.01\n",
      "Estimated Total Size (MB): 2.04\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "summary(model, (1000, 5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "adjacent-research",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict(brewery_name = 1687, review_aroma = 2.0, review_appearance = 2.5, review_palate = 2.5, review_taste = 2.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "present-meeting",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PytorchMultiClass(nn.Module):\n",
    "    def __init__(self, num_features):\n",
    "        super(PytorchMultiClass, self).__init__()\n",
    "        \n",
    "        self.layer_1 = nn.Linear(num_features, 32)\n",
    "        self.layer_out = nn.Linear(32, 104) #Number of classes in target variable?\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.dropout(F.relu(self.layer_1(x)), training=self.training)\n",
    "        x = self.layer_out(x)\n",
    "        return self.softmax(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "devoted-sauce",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = torch.load(\"../models/beer_pred.pt\", encoding='ascii')\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "independent-fisher",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "straight-exception",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "becoming-maria",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get target containing class indices back\n",
    "torch.argmax(data, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cardiovascular-receptor",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "becoming-intention",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = torch.load(\"../models/beer_pred.pt\", encoding='ascii')\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eight-metro",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchsummary import summary\n",
    "summary(model, (5, 104, 104))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "maritime-hydrogen",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "raw",
   "id": "portuguese-awareness",
   "metadata": {},
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from category_encoders.binary import BinaryEncoder\n",
    "pipe = Pipeline([\n",
    "    ('be', BinaryEncoder(cols=['brewery_name']))\n",
    "])"
   ]
  },
  {
   "cell_type": "raw",
   "id": "younger-photography",
   "metadata": {},
   "source": [
    "df_encoded = pipe.fit_transform(df_cleaned)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "meaning-presence",
   "metadata": {},
   "source": [
    "# Frequency encoder as alternative\n",
    "brewery_name_freq = df_cleaned['brewery_name'].value_counts().to_dict()\n",
    "df_cleaned['brewery_name'] = df_cleaned['brewery_name'].map(brewery_name_freq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "tracked-chess",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "raw",
   "id": "average-camera",
   "metadata": {},
   "source": [
    "# Import this class from src/models/pytorch and convert all sets to PytorchDataset\n",
    "#from src.models.pytorch import PytorchDataset\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class PytorchDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Pytorch dataset\n",
    "    ...\n",
    "\n",
    "    Attributes\n",
    "    ----------\n",
    "    X_tensor : Pytorch tensor\n",
    "        Features tensor\n",
    "    y_tensor : Pytorch tensor\n",
    "        Target tensor\n",
    "\n",
    "    Methods\n",
    "    -------\n",
    "    __getitem__(index)\n",
    "        Return features and target for a given index\n",
    "    __len__\n",
    "        Return the number of observations\n",
    "    to_tensor(data)\n",
    "        Convert Pandas Series to Pytorch tensor\n",
    "    \"\"\"\n",
    "        \n",
    "    def __init__(self, X, y):\n",
    "        self.X_tensor = self.to_tensor(X)\n",
    "        self.y_tensor = self.to_tensor(y)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        return self.X_tensor[index], self.y_tensor[index]\n",
    "        \n",
    "    def __len__ (self):\n",
    "        return len(self.X_tensor)\n",
    "    \n",
    "    def to_tensor(self, data):\n",
    "        return torch.Tensor(np.array(data))"
   ]
  },
  {
   "cell_type": "raw",
   "id": "bronze-lafayette",
   "metadata": {},
   "source": [
    "#from src.data.sets import split_sets_random, save_sets, pop_target\n",
    "\n",
    "def split_sets_random(df, target_col, test_ratio=0.2, to_numpy=False):\n",
    "    \"\"\"Split sets randomly\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    df : pd.DataFrame\n",
    "        Input dataframe\n",
    "    target_col : str\n",
    "        Name of the target column\n",
    "    test_ratio : float\n",
    "        Ratio used for the validation and testing sets (default: 0.2)\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    Numpy Array\n",
    "        Features for the training set\n",
    "    Numpy Array\n",
    "        Target for the training set\n",
    "    Numpy Array\n",
    "        Features for the validation set\n",
    "    Numpy Array\n",
    "        Target for the validation set\n",
    "    Numpy Array\n",
    "        Features for the testing set\n",
    "    Numpy Array\n",
    "        Target for the testing set\n",
    "    \"\"\"\n",
    "    \n",
    "    from sklearn.model_selection import train_test_split\n",
    "    \n",
    "    features, target = pop_target(df=df, target_col=target_col, to_numpy=to_numpy)\n",
    "    \n",
    "    X_data, X_test, y_data, y_test = train_test_split(features, target, test_size=test_ratio, random_state=8)\n",
    "    \n",
    "    val_ratio = test_ratio / (1 - test_ratio)\n",
    "    X_train, X_val, y_train, y_val = train_test_split(X_data, y_data, test_size=val_ratio, random_state=8)\n",
    "\n",
    "    return X_train, y_train, X_val, y_val, X_test, y_test\n",
    "\n",
    "\n",
    "def save_sets(X_train=None, y_train=None, X_val=None, y_val=None, X_test=None, y_test=None, path='../data/processed/'):\n",
    "    import numpy as np\n",
    "\n",
    "    if X_train is not None:\n",
    "      np.save(f'{path}X_train', X_train)\n",
    "    if X_val is not None:\n",
    "      np.save(f'{path}X_val',   X_val)\n",
    "    if X_test is not None:\n",
    "      np.save(f'{path}X_test',  X_test)\n",
    "    if y_train is not None:\n",
    "      np.save(f'{path}y_train', y_train)\n",
    "    if y_val is not None:\n",
    "      np.save(f'{path}y_val',   y_val)\n",
    "    if y_test is not None:\n",
    "      np.save(f'{path}y_test',  y_test)\n",
    "    \n",
    "def pop_target(df, target_col, to_numpy=False):\n",
    "    \"\"\"Extract target variable from dataframe and convert to nympy arrays if required\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    df : pd.DataFrame\n",
    "        Dataframe\n",
    "    target_col : str\n",
    "        Name of the target variable\n",
    "    to_numpy : bool\n",
    "        Flag stating to convert to numpy array or not\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    pd.DataFrame/Numpy array\n",
    "        Subsetted Pandas dataframe containing all features\n",
    "    pd.DataFrame/Numpy array\n",
    "        Subsetted Pandas dataframe containing the target\n",
    "    \"\"\"\n",
    "\n",
    "    df_copy = df.copy()\n",
    "    target = df_copy.pop(target_col)\n",
    "    \n",
    "    if to_numpy:\n",
    "        df_copy = df_copy.to_numpy()\n",
    "        target = target.to_numpy()\n",
    "    \n",
    "    return df_copy, target"
   ]
  },
  {
   "cell_type": "raw",
   "id": "trying-array",
   "metadata": {},
   "source": [
    "# Import NullModel from src.models.null\n",
    "#from src.models.null import NullModel\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "class NullModel:\n",
    "    \"\"\"\n",
    "    Class used as baseline model for both regression and classification\n",
    "    ...\n",
    "\n",
    "    Attributes\n",
    "    ----------\n",
    "    target_type : str\n",
    "        Type of ML problem (default regression)\n",
    "    y : Numpy Array-like\n",
    "        Target variable\n",
    "    pred_value : Float\n",
    "        Value to be used for prediction\n",
    "    preds : Numpy Array\n",
    "        Predicted array\n",
    "\n",
    "    Methods\n",
    "    -------\n",
    "    fit(y)\n",
    "        Store the input target variable and calculate the predicted value to be used based on the problem type\n",
    "    get_length\n",
    "        Calculate the number of observations from the target variable\n",
    "    predict(y)\n",
    "        Generate the predictions\n",
    "    fit_predict(y)\n",
    "        Perform a fit followed by predict\n",
    "    \"\"\"\n",
    "        \n",
    "    \n",
    "    def __init__(self, target_type: str = \"regression\"):\n",
    "        self.target_type = target_type\n",
    "        self.y = None\n",
    "        self.pred_value = None\n",
    "        self.preds = None\n",
    "        \n",
    "    def fit(self, y):\n",
    "        self.y = y\n",
    "        if self.target_type == \"regression\":\n",
    "            self.pred_value = y.mean()\n",
    "        else:\n",
    "            from scipy.stats import mode\n",
    "            self.pred_value = mode(y)[0][0]\n",
    "            \n",
    "    def get_length(self):\n",
    "        return len(self.y)\n",
    "    \n",
    "    def predict(self, y):\n",
    "        self.preds = np.full((self.get_length(), 1), self.pred_value)\n",
    "        return self.preds\n",
    "    \n",
    "    def fit_predict(self, y):\n",
    "        self.fit(y)\n",
    "        return self.predict(self.y)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "designed-grenada",
   "metadata": {},
   "source": [
    "# Import print_class_perf from src.models.performance\n",
    "#from src.models.performance import print_class_perf\n",
    "\n",
    "def print_class_perf(y_preds, y_actuals, set_name=None, average='binary'):\n",
    "    \"\"\"Print the Accuracy and F1 score for the provided data\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    y_preds : Numpy Array\n",
    "        Predicted target\n",
    "    y_actuals : Numpy Array\n",
    "        Actual target\n",
    "    set_name : str\n",
    "        Name of the set to be printed\n",
    "    average : str\n",
    "        Parameter  for F1-score averaging\n",
    "    Returns\n",
    "    -------\n",
    "    \"\"\"\n",
    "    from sklearn.metrics import accuracy_score\n",
    "    from sklearn.metrics import f1_score\n",
    "\n",
    "    print(f\"Accuracy {set_name}: {accuracy_score(y_actuals, y_preds)}\")\n",
    "    print(f\"F1 {set_name}: {f1_score(y_actuals, y_preds, average=average)}\")"
   ]
  },
  {
   "cell_type": "raw",
   "id": "expanded-seeker",
   "metadata": {},
   "source": [
    "# Import torch and torch.nn as nn\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "raw",
   "id": "documentary-sheffield",
   "metadata": {},
   "source": [
    "# Instantiate PytorchMultiClass with the correct number of input feature and save it into a variable called model\n",
    "#from src.models.pytorch import PytorchMultiClass\n",
    "\n",
    "class PytorchMultiClass(nn.Module):\n",
    "    def __init__(self, num_features):\n",
    "        super(PytorchMultiClass, self).__init__()\n",
    "        \n",
    "        self.layer_1 = nn.Linear(num_features, 32)\n",
    "        self.batchnorm_1 = nn.BatchNorm1d(32)\n",
    "        self.layer_2 = nn.Linear(32,16)\n",
    "        self.batchnorm_2 = nn.BatchNorm1d(16)\n",
    "        self.layer_3 = nn.Linear(16,8)\n",
    "        self.batchnorm_3 = nn.BatchNorm1d(8)\n",
    "        self.layer_out = nn.Linear(8, 104) #Number of classes in target variable\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.dropout(F.relu(self.layer_1(x)), training=self.training)\n",
    "        x = F.dropout(F.relu(self.layer_2(x)), training=self.training)\n",
    "        x = F.dropout(F.relu(self.layer_3(x)), training=self.training)\n",
    "        x = self.layer_out(x)\n",
    "        return self.softmax(x)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "moral-sister",
   "metadata": {},
   "source": [
    "# Set model to use the device available\n",
    "#from src.models.pytorch import get_device\n",
    "\n",
    "def get_device():\n",
    "    if torch.cuda.is_available():\n",
    "        device = torch.device('cuda:0')\n",
    "    else:\n",
    "        device = torch.device('cpu')\n",
    "    return device\n",
    "\n",
    "device = get_device()\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "federal-emphasis",
   "metadata": {},
   "source": [
    "# Create a function called train_classification() that will perform forward and back propagation and calculate loss and Accuracy scores\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "def train_classification(train_data, model, criterion, optimizer, batch_size, device, scheduler=None, generate_batch=None):\n",
    "    \"\"\"Train a Pytorch multi-class classification model\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    train_data : torch.utils.data.Dataset\n",
    "        Pytorch dataset\n",
    "    model: torch.nn.Module\n",
    "        Pytorch Model\n",
    "    criterion: function\n",
    "        Loss function\n",
    "    optimizer: torch.optim\n",
    "        Optimizer\n",
    "    bacth_size : int\n",
    "        Number of observations per batch\n",
    "    device : str\n",
    "        Name of the device used for the model\n",
    "    scheduler : torch.optim.lr_scheduler\n",
    "        Pytorch Scheduler used for updating learning rate\n",
    "    collate_fn : function\n",
    "        Function defining required pre-processing steps\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    Float\n",
    "        Loss score\n",
    "    Float:\n",
    "        Accuracy Score\n",
    "    \"\"\"\n",
    "    \n",
    "    # Set model to training mode\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    train_acc = 0\n",
    "    \n",
    "    # Create data loader\n",
    "    data = DataLoader(train_data, batch_size=batch_size, shuffle=True, collate_fn=generate_batch)\n",
    "    \n",
    "    # Iterate through data by batch of observations\n",
    "    for feature, target_class in data:\n",
    "\n",
    "        # Reset gradients\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Load data to specified device\n",
    "        feature, target_class = feature.to(device), target_class.to(device)\n",
    "        \n",
    "        # Make predictions\n",
    "        output = model(feature)\n",
    "        \n",
    "        # Calculate loss for given batch\n",
    "        loss = criterion(output, target_class.long())\n",
    "\n",
    "        # Calculate global loss\n",
    "        train_loss += loss.item()\n",
    "        \n",
    "        # Calculate gradients\n",
    "        loss.backward()\n",
    "\n",
    "        # Update Weights\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Calculate global accuracy\n",
    "        train_acc += (output.argmax(1) == target_class).sum().item()\n",
    "\n",
    "    # Adjust the learning rate\n",
    "    if scheduler:\n",
    "        scheduler.step()\n",
    "\n",
    "    return train_loss / len(train_data), train_acc / len(train_data)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "loved-gregory",
   "metadata": {},
   "source": [
    "# Create a function called test_classification() that will perform forward and calculate loss and accuracy scores\n",
    "def test_classification(test_data, model, criterion, batch_size, device, generate_batch=None):\n",
    "    \"\"\"Calculate performance of a Pytorch multi-class classification model\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    test_data : torch.utils.data.Dataset\n",
    "        Pytorch dataset\n",
    "    model: torch.nn.Module\n",
    "        Pytorch Model\n",
    "    criterion: function\n",
    "        Loss function\n",
    "    bacth_size : int\n",
    "        Number of observations per batch\n",
    "    device : str\n",
    "        Name of the device used for the model\n",
    "    collate_fn : function\n",
    "        Function defining required pre-processing steps\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    Float\n",
    "        Loss score\n",
    "    Float:\n",
    "        Accuracy Score\n",
    "    \"\"\"    \n",
    "    \n",
    "    # Set model to evaluation mode\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    test_acc = 0\n",
    "    \n",
    "    # Create data loader\n",
    "    data = DataLoader(test_data, batch_size=batch_size, collate_fn=generate_batch)\n",
    "    \n",
    "    # Iterate through data by batch of observations\n",
    "    for feature, target_class in data:\n",
    "        \n",
    "        # Load data to specified device\n",
    "        feature, target_class = feature.to(device), target_class.to(device)\n",
    "        \n",
    "        # Set no update to gradients\n",
    "        with torch.no_grad():\n",
    "            \n",
    "            # Make predictions\n",
    "            output = model(feature)\n",
    "            \n",
    "            # Calculate loss for given batch\n",
    "            loss = criterion(output, target_class.long())\n",
    "\n",
    "            # Calculate global loss\n",
    "            test_loss += loss.item()\n",
    "            \n",
    "            # Calculate global accuracy\n",
    "            test_acc += (output.argmax(1) == target_class).sum().item()\n",
    "\n",
    "    return test_loss / len(test_data), test_acc / len(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "vital-italian",
   "metadata": {},
   "outputs": [],
   "source": [
    "# make a class prediction for one row of data\n",
    "# adapted from https://machinelearningmastery.com/pytorch-tutorial-develop-deep-learning-models/\n",
    "from torch import Tensor\n",
    "from numpy import argmax\n",
    "\n",
    "def predict(brewery_name: int, review_aroma: int, review_appearance:int, review_palate: int, review_taste: int):\n",
    "    features = [brewery_name, review_aroma, review_appearance, review_palate, review_taste]\n",
    "    # convert row to data\n",
    "    features = Tensor([features])\n",
    "    # make prediction\n",
    "    yhat = model(features)\n",
    "    # retrieve numpy array\n",
    "    yhat = yhat.detach().numpy()\n",
    "    return argmax(yhat)\n",
    "    #return JSONResponse(pred.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "permanent-parish",
   "metadata": {},
   "outputs": [],
   "source": [
    "predict(brewery_name = 1687, review_aroma = 2.0, review_appearance = 2.5, review_palate = 2.5, review_taste = 2.5)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
