{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "emerging-essay",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "endangered-remark",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_url = 'https://raw.githubusercontent.com/aso-uts/applied_ds/master/unit3/dataset/Car%20Evaluation.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "presidential-germany",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Solution:\n",
    "df = pd.read_csv(file_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "boolean-climate",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>buying_price</th>\n",
       "      <th>maintenance_cost</th>\n",
       "      <th>doors</th>\n",
       "      <th>persons_capacity</th>\n",
       "      <th>luggage_boot</th>\n",
       "      <th>safety</th>\n",
       "      <th>evaluation</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>vhigh</td>\n",
       "      <td>vhigh</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>small</td>\n",
       "      <td>low</td>\n",
       "      <td>unacc</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>vhigh</td>\n",
       "      <td>vhigh</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>small</td>\n",
       "      <td>med</td>\n",
       "      <td>unacc</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>vhigh</td>\n",
       "      <td>vhigh</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>small</td>\n",
       "      <td>high</td>\n",
       "      <td>unacc</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>vhigh</td>\n",
       "      <td>vhigh</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>med</td>\n",
       "      <td>low</td>\n",
       "      <td>unacc</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>vhigh</td>\n",
       "      <td>vhigh</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>med</td>\n",
       "      <td>med</td>\n",
       "      <td>unacc</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  buying_price maintenance_cost doors persons_capacity luggage_boot safety  \\\n",
       "0        vhigh            vhigh     2                2        small    low   \n",
       "1        vhigh            vhigh     2                2        small    med   \n",
       "2        vhigh            vhigh     2                2        small   high   \n",
       "3        vhigh            vhigh     2                2          med    low   \n",
       "4        vhigh            vhigh     2                2          med    med   \n",
       "\n",
       "  evaluation  \n",
       "0      unacc  \n",
       "1      unacc  \n",
       "2      unacc  \n",
       "3      unacc  \n",
       "4      unacc  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Solution\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "similar-uncle",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cleaned = df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "prerequisite-spirituality",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solution\n",
    "cats_dict = {\n",
    "    'buying_price': [['low', 'med', 'high', 'vhigh']],\n",
    "    'maintenance_cost': [['low', 'med', 'high', 'vhigh']],\n",
    "    'doors': [['2', '3', '4', '5more']],\n",
    "    'persons_capacity': [['2', '4', 'more']],\n",
    "    'luggage_boot': [['small', 'med', 'big']],\n",
    "    'safety': [['low', 'med', 'high']],\n",
    "    'evaluation': [['unacc', 'acc', 'good', 'vgood']],\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "married-consent",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler, OrdinalEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "french-hotel",
   "metadata": {},
   "outputs": [],
   "source": [
    "for col, cats in cats_dict.items():\n",
    "    col_encoder = OrdinalEncoder(categories=cats)\n",
    "    df_cleaned[col] = col_encoder.fit_transform(df_cleaned[[col]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "handled-orchestra",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_cols = ['buying_price', 'maintenance_cost', 'doors', 'persons_capacity', 'luggage_boot', 'safety']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "going-columbia",
   "metadata": {},
   "outputs": [],
   "source": [
    "sc = StandardScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "caroline-criminal",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cleaned[num_cols] = sc.fit_transform(df_cleaned[num_cols])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "acute-bryan",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cleaned['evaluation'] = df_cleaned['evaluation'].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "proper-builder",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_sets_random(df, target_col, test_ratio=0.2, to_numpy=False):\n",
    "    \"\"\"Split sets randomly\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    df : pd.DataFrame\n",
    "        Input dataframe\n",
    "    target_col : str\n",
    "        Name of the target column\n",
    "    test_ratio : float\n",
    "        Ratio used for the validation and testing sets (default: 0.2)\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    Numpy Array\n",
    "        Features for the training set\n",
    "    Numpy Array\n",
    "        Target for the training set\n",
    "    Numpy Array\n",
    "        Features for the validation set\n",
    "    Numpy Array\n",
    "        Target for the validation set\n",
    "    Numpy Array\n",
    "        Features for the testing set\n",
    "    Numpy Array\n",
    "        Target for the testing set\n",
    "    \"\"\"\n",
    "    \n",
    "    from sklearn.model_selection import train_test_split\n",
    "    \n",
    "    features, target = pop_target(df=df, target_col=target_col, to_numpy=to_numpy)\n",
    "    \n",
    "    X_data, X_test, y_data, y_test = train_test_split(features, target, test_size=test_ratio, random_state=8)\n",
    "    \n",
    "    val_ratio = test_ratio / (1 - test_ratio)\n",
    "    X_train, X_val, y_train, y_val = train_test_split(X_data, y_data, test_size=val_ratio, random_state=8)\n",
    "\n",
    "    return X_train, y_train, X_val, y_val, X_test, y_test\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "premier-thumb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_sets(X_train=None, y_train=None, X_val=None, y_val=None, X_test=None, y_test=None, path='../data/processed/'):\n",
    "    import numpy as np\n",
    "\n",
    "    if X_train is not None:\n",
    "      np.save(f'{path}X_train', X_train)\n",
    "    if X_val is not None:\n",
    "      np.save(f'{path}X_val',   X_val)\n",
    "    if X_test is not None:\n",
    "      np.save(f'{path}X_test',  X_test)\n",
    "    if y_train is not None:\n",
    "      np.save(f'{path}y_train', y_train)\n",
    "    if y_val is not None:\n",
    "      np.save(f'{path}y_val',   y_val)\n",
    "    if y_test is not None:\n",
    "      np.save(f'{path}y_test',  y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "consolidated-anthony",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pop_target(df, target_col, to_numpy=False):\n",
    "    \"\"\"Extract target variable from dataframe and convert to nympy arrays if required\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    df : pd.DataFrame\n",
    "        Dataframe\n",
    "    target_col : str\n",
    "        Name of the target variable\n",
    "    to_numpy : bool\n",
    "        Flag stating to convert to numpy array or not\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    pd.DataFrame/Numpy array\n",
    "        Subsetted Pandas dataframe containing all features\n",
    "    pd.DataFrame/Numpy array\n",
    "        Subsetted Pandas dataframe containing the target\n",
    "    \"\"\"\n",
    "\n",
    "    df_copy = df.copy()\n",
    "    target = df_copy.pop(target_col)\n",
    "    \n",
    "    if to_numpy:\n",
    "        df_copy = df_copy.to_numpy()\n",
    "        target = target.to_numpy()\n",
    "    \n",
    "    return df_copy, target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "tutorial-shuttle",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, y_train, X_val, y_val, X_test, y_test = split_sets_random(df_cleaned, target_col='evaluation', test_ratio=0.2, to_numpy=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ruled-peter",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "class PytorchDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Pytorch dataset\n",
    "    ...\n",
    "\n",
    "    Attributes\n",
    "    ----------\n",
    "    X_tensor : Pytorch tensor\n",
    "        Features tensor\n",
    "    y_tensor : Pytorch tensor\n",
    "        Target tensor\n",
    "\n",
    "    Methods\n",
    "    -------\n",
    "    __getitem__(index)\n",
    "        Return features and target for a given index\n",
    "    __len__\n",
    "        Return the number of observations\n",
    "    to_tensor(data)\n",
    "        Convert Pandas Series to Pytorch tensor\n",
    "    \"\"\"\n",
    "        \n",
    "    def __init__(self, X, y):\n",
    "        self.X_tensor = self.to_tensor(X)\n",
    "        self.y_tensor = self.to_tensor(y)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        return self.X_tensor[index], self.y_tensor[index]\n",
    "        \n",
    "    def __len__ (self):\n",
    "        return len(self.X_tensor)\n",
    "    \n",
    "    def to_tensor(self, data):\n",
    "        return torch.Tensor(np.array(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "finnish-button",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "under-wagon",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = PytorchDataset(X=X_train, y=y_train)\n",
    "val_dataset = PytorchDataset(X=X_val, y=y_val)\n",
    "test_dataset = PytorchDataset(X=X_test, y=y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "above-christianity",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "class NullModel:\n",
    "    \"\"\"\n",
    "    Class used as baseline model for both regression and classification\n",
    "    ...\n",
    "\n",
    "    Attributes\n",
    "    ----------\n",
    "    target_type : str\n",
    "        Type of ML problem (default regression)\n",
    "    y : Numpy Array-like\n",
    "        Target variable\n",
    "    pred_value : Float\n",
    "        Value to be used for prediction\n",
    "    preds : Numpy Array\n",
    "        Predicted array\n",
    "\n",
    "    Methods\n",
    "    -------\n",
    "    fit(y)\n",
    "        Store the input target variable and calculate the predicted value to be used based on the problem type\n",
    "    get_length\n",
    "        Calculate the number of observations from the target variable\n",
    "    predict(y)\n",
    "        Generate the predictions\n",
    "    fit_predict(y)\n",
    "        Perform a fit followed by predict\n",
    "    \"\"\"\n",
    "        \n",
    "    \n",
    "    def __init__(self, target_type: str = \"regression\"):\n",
    "        self.target_type = target_type\n",
    "        self.y = None\n",
    "        self.pred_value = None\n",
    "        self.preds = None\n",
    "        \n",
    "    def fit(self, y):\n",
    "        self.y = y\n",
    "        if self.target_type == \"regression\":\n",
    "            self.pred_value = y.mean()\n",
    "        else:\n",
    "            from scipy.stats import mode\n",
    "            self.pred_value = mode(y)[0][0]\n",
    "            \n",
    "    def get_length(self):\n",
    "        return len(self.y)\n",
    "    \n",
    "    def predict(self, y):\n",
    "        self.preds = np.full((self.get_length(), 1), self.pred_value)\n",
    "        return self.preds\n",
    "    \n",
    "    def fit_predict(self, y):\n",
    "        self.fit(y)\n",
    "        return self.predict(self.y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "marked-affiliate",
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline_model = NullModel(target_type='classification')\n",
    "y_base = baseline_model.fit_predict(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "limiting-template",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_class_perf(y_preds, y_actuals, set_name=None, average='binary'):\n",
    "    \"\"\"Print the Accuracy and F1 score for the provided data\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    y_preds : Numpy Array\n",
    "        Predicted target\n",
    "    y_actuals : Numpy Array\n",
    "        Actual target\n",
    "    set_name : str\n",
    "        Name of the set to be printed\n",
    "    average : str\n",
    "        Parameter  for F1-score averaging\n",
    "    Returns\n",
    "    -------\n",
    "    \"\"\"\n",
    "    from sklearn.metrics import accuracy_score\n",
    "    from sklearn.metrics import f1_score\n",
    "\n",
    "    print(f\"Accuracy {set_name}: {accuracy_score(y_actuals, y_preds)}\")\n",
    "    print(f\"F1 {set_name}: {f1_score(y_actuals, y_preds, average=average)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "expressed-skiing",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy Training: 0.6988416988416989\n",
      "F1 Training: 0.5749561249561249\n"
     ]
    }
   ],
   "source": [
    "print_class_perf(y_base, y_train, set_name='Training', average='weighted')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "helpful-manhattan",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "entitled-dylan",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PytorchMultiClass(nn.Module):\n",
    "    def __init__(self, num_features):\n",
    "        super(PytorchMultiClass, self).__init__()\n",
    "        \n",
    "        self.layer_1 = nn.Linear(num_features, 32)\n",
    "        self.layer_out = nn.Linear(32, 4)\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.dropout(F.relu(self.layer_1(x)), training=self.training)\n",
    "        x = self.layer_out(x)\n",
    "        return self.softmax(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "valued-cathedral",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = PytorchMultiClass(X_train.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "expanded-morocco",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_device():\n",
    "    if torch.cuda.is_available():\n",
    "        device = torch.device('cuda:0')\n",
    "    else:\n",
    "        device = torch.device('cpu') # don't have GPU \n",
    "    return device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "controlling-gospel",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PytorchMultiClass(\n",
       "  (layer_1): Linear(in_features=6, out_features=32, bias=True)\n",
       "  (layer_out): Linear(in_features=32, out_features=4, bias=True)\n",
       "  (softmax): Softmax(dim=1)\n",
       ")"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = get_device()\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "caroline-pharmaceutical",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "unlike-vector",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "dying-affairs",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_classification(train_data, model, criterion, optimizer, batch_size, device, scheduler=None, generate_batch=None):\n",
    "    \"\"\"Train a Pytorch multi-class classification model\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    train_data : torch.utils.data.Dataset\n",
    "        Pytorch dataset\n",
    "    model: torch.nn.Module\n",
    "        Pytorch Model\n",
    "    criterion: function\n",
    "        Loss function\n",
    "    optimizer: torch.optim\n",
    "        Optimizer\n",
    "    bacth_size : int\n",
    "        Number of observations per batch\n",
    "    device : str\n",
    "        Name of the device used for the model\n",
    "    scheduler : torch.optim.lr_scheduler\n",
    "        Pytorch Scheduler used for updating learning rate\n",
    "    collate_fn : function\n",
    "        Function defining required pre-processing steps\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    Float\n",
    "        Loss score\n",
    "    Float:\n",
    "        Accuracy Score\n",
    "    \"\"\"\n",
    "    \n",
    "    # Set model to training mode\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    train_acc = 0\n",
    "    \n",
    "    # Create data loader\n",
    "    data = DataLoader(train_data, batch_size=batch_size, shuffle=True, collate_fn=generate_batch)\n",
    "    \n",
    "    # Iterate through data by batch of observations\n",
    "    for feature, target_class in data:\n",
    "\n",
    "        # Reset gradients\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Load data to specified device\n",
    "        feature, target_class = feature.to(device), target_class.to(device)\n",
    "        \n",
    "        # Make predictions\n",
    "        output = model(feature)\n",
    "        \n",
    "        # Calculate loss for given batch\n",
    "        loss = criterion(output, target_class.long())\n",
    "\n",
    "        # Calculate global loss\n",
    "        train_loss += loss.item()\n",
    "        \n",
    "        # Calculate gradients\n",
    "        loss.backward()\n",
    "\n",
    "        # Update Weights\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Calculate global accuracy\n",
    "        train_acc += (output.argmax(1) == target_class).sum().item()\n",
    "\n",
    "    # Adjust the learning rate\n",
    "    if scheduler:\n",
    "        scheduler.step()\n",
    "\n",
    "    return train_loss / len(train_data), train_acc / len(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "muslim-reservation",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_classification(test_data, model, criterion, batch_size, device, generate_batch=None):\n",
    "    \"\"\"Calculate performance of a Pytorch multi-class classification model\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    test_data : torch.utils.data.Dataset\n",
    "        Pytorch dataset\n",
    "    model: torch.nn.Module\n",
    "        Pytorch Model\n",
    "    criterion: function\n",
    "        Loss function\n",
    "    bacth_size : int\n",
    "        Number of observations per batch\n",
    "    device : str\n",
    "        Name of the device used for the model\n",
    "    collate_fn : function\n",
    "        Function defining required pre-processing steps\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    Float\n",
    "        Loss score\n",
    "    Float:\n",
    "        Accuracy Score\n",
    "    \"\"\"    \n",
    "    \n",
    "    # Set model to evaluation mode\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    test_acc = 0\n",
    "    \n",
    "    # Create data loader\n",
    "    data = DataLoader(test_data, batch_size=batch_size, collate_fn=generate_batch)\n",
    "    \n",
    "    # Iterate through data by batch of observations\n",
    "    for feature, target_class in data:\n",
    "        \n",
    "        # Load data to specified device\n",
    "        feature, target_class = feature.to(device), target_class.to(device)\n",
    "        \n",
    "        # Set no update to gradients\n",
    "        with torch.no_grad():\n",
    "            \n",
    "            # Make predictions\n",
    "            output = model(feature)\n",
    "            \n",
    "            # Calculate loss for given batch\n",
    "            loss = criterion(output, target_class.long())\n",
    "\n",
    "            # Calculate global loss\n",
    "            test_loss += loss.item()\n",
    "            \n",
    "            # Calculate global accuracy\n",
    "            test_acc += (output.argmax(1) == target_class).sum().item()\n",
    "\n",
    "    return test_loss / len(test_data), test_acc / len(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "interstate-check",
   "metadata": {},
   "outputs": [],
   "source": [
    "N_EPOCHS = 50\n",
    "BATCH_SIZE = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "exempt-brush",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0\n",
      "\t(train)\t|\tLoss: 0.0315\t|\tAcc: 75.2%\n",
      "\t(valid)\t|\tLoss: 0.0295\t|\tAcc: 80.9%\n",
      "Epoch: 1\n",
      "\t(train)\t|\tLoss: 0.0302\t|\tAcc: 79.3%\n",
      "\t(valid)\t|\tLoss: 0.0294\t|\tAcc: 81.2%\n",
      "Epoch: 2\n",
      "\t(train)\t|\tLoss: 0.0294\t|\tAcc: 82.0%\n",
      "\t(valid)\t|\tLoss: 0.0289\t|\tAcc: 83.8%\n",
      "Epoch: 3\n",
      "\t(train)\t|\tLoss: 0.0293\t|\tAcc: 82.3%\n",
      "\t(valid)\t|\tLoss: 0.0289\t|\tAcc: 84.1%\n",
      "Epoch: 4\n",
      "\t(train)\t|\tLoss: 0.0290\t|\tAcc: 83.4%\n",
      "\t(valid)\t|\tLoss: 0.0273\t|\tAcc: 88.7%\n",
      "Epoch: 5\n",
      "\t(train)\t|\tLoss: 0.0286\t|\tAcc: 84.2%\n",
      "\t(valid)\t|\tLoss: 0.0285\t|\tAcc: 84.4%\n",
      "Epoch: 6\n",
      "\t(train)\t|\tLoss: 0.0297\t|\tAcc: 81.2%\n",
      "\t(valid)\t|\tLoss: 0.0289\t|\tAcc: 83.5%\n",
      "Epoch: 7\n",
      "\t(train)\t|\tLoss: 0.0294\t|\tAcc: 82.3%\n",
      "\t(valid)\t|\tLoss: 0.0284\t|\tAcc: 85.3%\n",
      "Epoch: 8\n",
      "\t(train)\t|\tLoss: 0.0294\t|\tAcc: 81.7%\n",
      "\t(valid)\t|\tLoss: 0.0288\t|\tAcc: 83.5%\n",
      "Epoch: 9\n",
      "\t(train)\t|\tLoss: 0.0282\t|\tAcc: 85.5%\n",
      "\t(valid)\t|\tLoss: 0.0273\t|\tAcc: 88.4%\n",
      "Epoch: 10\n",
      "\t(train)\t|\tLoss: 0.0281\t|\tAcc: 86.1%\n",
      "\t(valid)\t|\tLoss: 0.0285\t|\tAcc: 84.7%\n",
      "Epoch: 11\n",
      "\t(train)\t|\tLoss: 0.0285\t|\tAcc: 84.7%\n",
      "\t(valid)\t|\tLoss: 0.0275\t|\tAcc: 88.2%\n",
      "Epoch: 12\n",
      "\t(train)\t|\tLoss: 0.0282\t|\tAcc: 85.6%\n",
      "\t(valid)\t|\tLoss: 0.0275\t|\tAcc: 87.9%\n",
      "Epoch: 13\n",
      "\t(train)\t|\tLoss: 0.0281\t|\tAcc: 86.1%\n",
      "\t(valid)\t|\tLoss: 0.0280\t|\tAcc: 86.1%\n",
      "Epoch: 14\n",
      "\t(train)\t|\tLoss: 0.0286\t|\tAcc: 84.5%\n",
      "\t(valid)\t|\tLoss: 0.0295\t|\tAcc: 81.5%\n",
      "Epoch: 15\n",
      "\t(train)\t|\tLoss: 0.0291\t|\tAcc: 82.7%\n",
      "\t(valid)\t|\tLoss: 0.0276\t|\tAcc: 87.9%\n",
      "Epoch: 16\n",
      "\t(train)\t|\tLoss: 0.0277\t|\tAcc: 87.0%\n",
      "\t(valid)\t|\tLoss: 0.0279\t|\tAcc: 86.7%\n",
      "Epoch: 17\n",
      "\t(train)\t|\tLoss: 0.0282\t|\tAcc: 85.9%\n",
      "\t(valid)\t|\tLoss: 0.0283\t|\tAcc: 85.3%\n",
      "Epoch: 18\n",
      "\t(train)\t|\tLoss: 0.0283\t|\tAcc: 85.3%\n",
      "\t(valid)\t|\tLoss: 0.0283\t|\tAcc: 85.5%\n",
      "Epoch: 19\n",
      "\t(train)\t|\tLoss: 0.0278\t|\tAcc: 87.0%\n",
      "\t(valid)\t|\tLoss: 0.0277\t|\tAcc: 87.0%\n",
      "Epoch: 20\n",
      "\t(train)\t|\tLoss: 0.0275\t|\tAcc: 87.8%\n",
      "\t(valid)\t|\tLoss: 0.0276\t|\tAcc: 87.3%\n",
      "Epoch: 21\n",
      "\t(train)\t|\tLoss: 0.0275\t|\tAcc: 87.6%\n",
      "\t(valid)\t|\tLoss: 0.0274\t|\tAcc: 88.2%\n",
      "Epoch: 22\n",
      "\t(train)\t|\tLoss: 0.0280\t|\tAcc: 86.5%\n",
      "\t(valid)\t|\tLoss: 0.0282\t|\tAcc: 85.5%\n",
      "Epoch: 23\n",
      "\t(train)\t|\tLoss: 0.0279\t|\tAcc: 86.6%\n",
      "\t(valid)\t|\tLoss: 0.0281\t|\tAcc: 85.8%\n",
      "Epoch: 24\n",
      "\t(train)\t|\tLoss: 0.0280\t|\tAcc: 86.8%\n",
      "\t(valid)\t|\tLoss: 0.0275\t|\tAcc: 87.9%\n",
      "Epoch: 25\n",
      "\t(train)\t|\tLoss: 0.0278\t|\tAcc: 86.8%\n",
      "\t(valid)\t|\tLoss: 0.0276\t|\tAcc: 87.6%\n",
      "Epoch: 26\n",
      "\t(train)\t|\tLoss: 0.0277\t|\tAcc: 87.4%\n",
      "\t(valid)\t|\tLoss: 0.0278\t|\tAcc: 87.0%\n",
      "Epoch: 27\n",
      "\t(train)\t|\tLoss: 0.0277\t|\tAcc: 87.5%\n",
      "\t(valid)\t|\tLoss: 0.0274\t|\tAcc: 88.2%\n",
      "Epoch: 28\n",
      "\t(train)\t|\tLoss: 0.0275\t|\tAcc: 87.7%\n",
      "\t(valid)\t|\tLoss: 0.0276\t|\tAcc: 87.6%\n",
      "Epoch: 29\n",
      "\t(train)\t|\tLoss: 0.0278\t|\tAcc: 87.0%\n",
      "\t(valid)\t|\tLoss: 0.0277\t|\tAcc: 87.3%\n",
      "Epoch: 30\n",
      "\t(train)\t|\tLoss: 0.0278\t|\tAcc: 87.5%\n",
      "\t(valid)\t|\tLoss: 0.0274\t|\tAcc: 88.2%\n",
      "Epoch: 31\n",
      "\t(train)\t|\tLoss: 0.0275\t|\tAcc: 88.3%\n",
      "\t(valid)\t|\tLoss: 0.0276\t|\tAcc: 87.6%\n",
      "Epoch: 32\n",
      "\t(train)\t|\tLoss: 0.0275\t|\tAcc: 87.8%\n",
      "\t(valid)\t|\tLoss: 0.0277\t|\tAcc: 87.0%\n",
      "Epoch: 33\n",
      "\t(train)\t|\tLoss: 0.0275\t|\tAcc: 87.8%\n",
      "\t(valid)\t|\tLoss: 0.0278\t|\tAcc: 87.0%\n",
      "Epoch: 34\n",
      "\t(train)\t|\tLoss: 0.0279\t|\tAcc: 87.4%\n",
      "\t(valid)\t|\tLoss: 0.0279\t|\tAcc: 86.7%\n",
      "Epoch: 35\n",
      "\t(train)\t|\tLoss: 0.0282\t|\tAcc: 85.7%\n",
      "\t(valid)\t|\tLoss: 0.0282\t|\tAcc: 85.8%\n",
      "Epoch: 36\n",
      "\t(train)\t|\tLoss: 0.0280\t|\tAcc: 86.7%\n",
      "\t(valid)\t|\tLoss: 0.0279\t|\tAcc: 86.4%\n",
      "Epoch: 37\n",
      "\t(train)\t|\tLoss: 0.0277\t|\tAcc: 87.4%\n",
      "\t(valid)\t|\tLoss: 0.0273\t|\tAcc: 88.4%\n",
      "Epoch: 38\n",
      "\t(train)\t|\tLoss: 0.0283\t|\tAcc: 85.4%\n",
      "\t(valid)\t|\tLoss: 0.0279\t|\tAcc: 86.7%\n",
      "Epoch: 39\n",
      "\t(train)\t|\tLoss: 0.0275\t|\tAcc: 87.9%\n",
      "\t(valid)\t|\tLoss: 0.0279\t|\tAcc: 86.4%\n",
      "Epoch: 40\n",
      "\t(train)\t|\tLoss: 0.0275\t|\tAcc: 87.7%\n",
      "\t(valid)\t|\tLoss: 0.0273\t|\tAcc: 88.4%\n",
      "Epoch: 41\n",
      "\t(train)\t|\tLoss: 0.0273\t|\tAcc: 88.3%\n",
      "\t(valid)\t|\tLoss: 0.0273\t|\tAcc: 88.4%\n",
      "Epoch: 42\n",
      "\t(train)\t|\tLoss: 0.0273\t|\tAcc: 88.7%\n",
      "\t(valid)\t|\tLoss: 0.0273\t|\tAcc: 88.4%\n",
      "Epoch: 43\n",
      "\t(train)\t|\tLoss: 0.0277\t|\tAcc: 87.2%\n",
      "\t(valid)\t|\tLoss: 0.0276\t|\tAcc: 87.9%\n",
      "Epoch: 44\n",
      "\t(train)\t|\tLoss: 0.0285\t|\tAcc: 84.9%\n",
      "\t(valid)\t|\tLoss: 0.0294\t|\tAcc: 81.5%\n",
      "Epoch: 45\n",
      "\t(train)\t|\tLoss: 0.0292\t|\tAcc: 82.4%\n",
      "\t(valid)\t|\tLoss: 0.0288\t|\tAcc: 83.8%\n",
      "Epoch: 46\n",
      "\t(train)\t|\tLoss: 0.0288\t|\tAcc: 83.8%\n",
      "\t(valid)\t|\tLoss: 0.0284\t|\tAcc: 85.3%\n",
      "Epoch: 47\n",
      "\t(train)\t|\tLoss: 0.0283\t|\tAcc: 85.6%\n",
      "\t(valid)\t|\tLoss: 0.0272\t|\tAcc: 89.0%\n",
      "Epoch: 48\n",
      "\t(train)\t|\tLoss: 0.0281\t|\tAcc: 86.5%\n",
      "\t(valid)\t|\tLoss: 0.0270\t|\tAcc: 89.6%\n",
      "Epoch: 49\n",
      "\t(train)\t|\tLoss: 0.0282\t|\tAcc: 86.2%\n",
      "\t(valid)\t|\tLoss: 0.0278\t|\tAcc: 86.4%\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(N_EPOCHS):\n",
    "    train_loss, train_acc = train_classification(train_dataset, model=model, criterion=criterion, optimizer=optimizer, batch_size=BATCH_SIZE, device=device)\n",
    "    valid_loss, valid_acc = test_classification(val_dataset, model=model, criterion=criterion, batch_size=BATCH_SIZE, device=device)\n",
    "\n",
    "    print(f'Epoch: {epoch}')\n",
    "    print(f'\\t(train)\\t|\\tLoss: {train_loss:.4f}\\t|\\tAcc: {train_acc * 100:.1f}%')\n",
    "    print(f'\\t(valid)\\t|\\tLoss: {valid_loss:.4f}\\t|\\tAcc: {valid_acc * 100:.1f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "warming-opinion",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 0.0286\t|\tAccuracy: 0.8\n"
     ]
    }
   ],
   "source": [
    "test_loss, test_acc = test_classification(test_dataset, model=model, criterion=criterion, batch_size=BATCH_SIZE, device=device)\n",
    "print(f'\\tLoss: {test_loss:.4f}\\t|\\tAccuracy: {test_acc:.1f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "joined-placement",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "concerned-pierce",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
